{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development for NetTrafficGuard\n",
    "\n",
    "In this notebook, we will develop machine learning models to analyze and enhance network traffic security. We'll use three processed datasets for our analysis:\n",
    "1. **CICIDS2017_processed.csv**\n",
    "2. **KDDCup1999_processed.csv**\n",
    "3. **NSL-KDD_processed.csv**\n",
    "\n",
    "The primary steps include:\n",
    "- Loading and merging the datasets\n",
    "- Feature engineering\n",
    "- Splitting the data into training and test sets\n",
    "- Building and evaluating various machine learning models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\processedCICIDS2017_Cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cicids2017_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mHackatone Project\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mNetTrafficGuard\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mprocessed\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mprocessedCICIDS2017_Cleaned.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m kddcup1999_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mHackatone Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNetTrafficGuard\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKDDCup1999_Cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m nsl_kdd_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mHackatone Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNetTrafficGuard\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNSL-KDD_Cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\processedCICIDS2017_Cleaned.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "cicids2017_df = pd.read_csv('E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\processedCICIDS2017_Cleaned.csv')\n",
    "kddcup1999_df = pd.read_csv('E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\KDDCup1999_Cleaned.csv')\n",
    "nsl_kdd_df = pd.read_csv('E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\NSL-KDD_Cleaned.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"CICIDS2017 DataFrame:\")\n",
    "print(cicids2017_df.info())\n",
    "print(cicids2017_df.head())\n",
    "\n",
    "print(\"\\nKDDCup1999 DataFrame:\")\n",
    "print(kddcup1999_df.info())\n",
    "print(kddcup1999_df.head())\n",
    "\n",
    "print(\"\\nNSL-KDD DataFrame:\")\n",
    "print(nsl_kdd_df.info())\n",
    "print(nsl_kdd_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Inspection and Preprocessing**\n",
    "\n",
    "We will inspect the loaded data to understand its structure, and then preprocess it to ensure consistency across the datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Inspection and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in CICIDS2017:\n",
      "Flow ID     0\n",
      "Src IP      0\n",
      "Src Port    0\n",
      "Dst IP      0\n",
      "Dst Port    0\n",
      "           ..\n",
      "Idle Std    0\n",
      "Idle Max    0\n",
      "Idle Min    0\n",
      "Label       0\n",
      "Label.1     0\n",
      "Length: 85, dtype: int64\n",
      "\n",
      "Missing values in KDDCup1999:\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "27    0\n",
      "28    0\n",
      "29    0\n",
      "30    0\n",
      "31    0\n",
      "32    0\n",
      "33    0\n",
      "34    0\n",
      "35    0\n",
      "36    0\n",
      "37    0\n",
      "38    0\n",
      "39    0\n",
      "40    0\n",
      "41    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in NSL-KDD:\n",
      "0          0\n",
      "tcp        0\n",
      "private    0\n",
      "rej        0\n",
      "0.1        0\n",
      "0.2        0\n",
      "0.3        0\n",
      "0.4        0\n",
      "0.5        0\n",
      "0.6        0\n",
      "0.7        0\n",
      "0.8        0\n",
      "0.9        0\n",
      "0.10       0\n",
      "0.11       0\n",
      "0.12       0\n",
      "0.13       0\n",
      "0.14       0\n",
      "0.15       0\n",
      "0.16       0\n",
      "0.17       0\n",
      "0.18       0\n",
      "229        0\n",
      "10         0\n",
      "0.19       0\n",
      "0.20       0\n",
      "1          0\n",
      "1.1        0\n",
      "0.04       0\n",
      "0.06       0\n",
      "0.21       0\n",
      "255        0\n",
      "10.1       0\n",
      "0.04.1     0\n",
      "0.06.1     0\n",
      "0.22       0\n",
      "0.23       0\n",
      "0.24       0\n",
      "0.25       0\n",
      "1.2        0\n",
      "1.3        0\n",
      "neptune    0\n",
      "21         0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned CICIDS2017 DataFrame:\n",
      "                                   flow_id         src_ip  src_port  \\\n",
      "0  10.152.152.11-216.58.220.99-57158-443-6  10.152.152.11   57158.0   \n",
      "1  10.152.152.11-216.58.220.99-57159-443-6  10.152.152.11   57159.0   \n",
      "2  10.152.152.11-216.58.220.99-57160-443-6  10.152.152.11   57160.0   \n",
      "\n",
      "          dst_ip  dst_port  protocol               timestamp  flow_duration  \\\n",
      "0  216.58.220.99     443.0       6.0  24/07/2015 04:09:48 PM          229.0   \n",
      "1  216.58.220.99     443.0       6.0  24/07/2015 04:09:48 PM          407.0   \n",
      "2  216.58.220.99     443.0       6.0  24/07/2015 04:09:48 PM          431.0   \n",
      "\n",
      "   total_fwd_packet  total_bwd_packets  ...  active_mean  active_std  \\\n",
      "0               1.0                1.0  ...          0.0         0.0   \n",
      "1               1.0                1.0  ...          0.0         0.0   \n",
      "2               1.0                1.0  ...          0.0         0.0   \n",
      "\n",
      "   active_max  active_min  idle_mean  idle_std  idle_max  idle_min    label  \\\n",
      "0         0.0         0.0        0.0       0.0       0.0       0.0  Non-Tor   \n",
      "1         0.0         0.0        0.0       0.0       0.0       0.0  Non-Tor   \n",
      "2         0.0         0.0        0.0       0.0       0.0       0.0  Non-Tor   \n",
      "\n",
      "           label.1  \n",
      "0  AUDIO-STREAMING  \n",
      "1  AUDIO-STREAMING  \n",
      "2  AUDIO-STREAMING  \n",
      "\n",
      "[3 rows x 85 columns]\n",
      "\n",
      "Cleaned KDDCup1999 DataFrame:\n",
      "   0     1      2   3     4  5  6  7  8  9  ...   32    33    34    35   36  \\\n",
      "0  0  icmp  ecr_i  SF  1032  0  0  0  0  0  ...  113  0.44  0.01  0.44  0.0   \n",
      "1  0  icmp  ecr_i  SF  1032  0  0  0  0  0  ...  123  0.48  0.01  0.48  0.0   \n",
      "2  0  icmp  ecr_i  SF  1032  0  0  0  0  0  ...  133  0.52  0.01  0.52  0.0   \n",
      "\n",
      "    37   38   39   40      41  \n",
      "0  0.0  0.0  0.0  0.0  smurf.  \n",
      "1  0.0  0.0  0.0  0.0  smurf.  \n",
      "2  0.0  0.0  0.0  0.0  smurf.  \n",
      "\n",
      "[3 rows x 42 columns]\n",
      "\n",
      "Cleaned NSL-KDD DataFrame:\n",
      "     0   tcp   private  rej      0.1  0.2  0.3  0.4  0.5  0.6  ...  0.04.1  \\\n",
      "0  0.0   tcp   private  REJ      0.0  0.0  0.0  0.0  0.0  0.0  ...    0.00   \n",
      "1  2.0   tcp  ftp_data   SF  12983.0  0.0  0.0  0.0  0.0  0.0  ...    0.61   \n",
      "2  0.0  icmp     eco_i   SF     20.0  0.0  0.0  0.0  0.0  0.0  ...    1.00   \n",
      "\n",
      "   0.06.1  0.22  0.23  0.24  0.25  1.2  1.3  neptune    21  \n",
      "0    0.06  0.00  0.00   0.0   0.0  1.0  1.0  neptune  21.0  \n",
      "1    0.04  0.61  0.02   0.0   0.0  0.0  0.0   normal  21.0  \n",
      "2    0.00  1.00  0.28   0.0   0.0  0.0  0.0    saint  15.0  \n",
      "\n",
      "[3 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in CICIDS2017:\")\n",
    "print(cicids2017_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in KDDCup1999:\")\n",
    "print(kddcup1999_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in NSL-KDD:\")\n",
    "print(nsl_kdd_df.isnull().sum())\n",
    "\n",
    "# Drop columns with excessive missing values or irrelevance\n",
    "cicids2017_df.dropna(axis=1, thresh=0.9*len(cicids2017_df), inplace=True)\n",
    "kddcup1999_df.dropna(axis=1, thresh=0.9*len(kddcup1999_df), inplace=True)\n",
    "nsl_kdd_df.dropna(axis=1, thresh=0.9*len(nsl_kdd_df), inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "cicids2017_df.drop_duplicates(inplace=True)\n",
    "kddcup1999_df.drop_duplicates(inplace=True)\n",
    "nsl_kdd_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Example of further cleaning: standardizing column names\n",
    "cicids2017_df.columns = cicids2017_df.columns.str.lower().str.replace(' ', '_')\n",
    "kddcup1999_df.columns = kddcup1999_df.columns.str.lower().str.replace(' ', '_')\n",
    "nsl_kdd_df.columns = nsl_kdd_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "print(\"\\nCleaned CICIDS2017 DataFrame:\")\n",
    "print(cicids2017_df.head(3))\n",
    "print(\"\\nCleaned KDDCup1999 DataFrame:\")\n",
    "print(kddcup1999_df.head(3))\n",
    "print(\"\\nCleaned NSL-KDD DataFrame:\")\n",
    "print(nsl_kdd_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled CICIDS2017 DataFrame:\n",
      "    flow_id    src_ip  src_port    dst_ip  dst_port  protocol  timestamp  \\\n",
      "0 -0.686773 -0.417225  0.966619  0.452353  -0.66708 -0.865321   0.769594   \n",
      "1 -0.686728 -0.417225  0.966671  0.452353  -0.66708 -0.865321   0.769594   \n",
      "\n",
      "   flow_duration  total_fwd_packet  total_bwd_packets  ...  active_mean  \\\n",
      "0      -0.533407         -0.162008          -0.154376  ...          0.0   \n",
      "1      -0.533402         -0.162008          -0.154376  ...          0.0   \n",
      "\n",
      "   active_std  active_max  active_min  idle_mean  idle_std  idle_max  \\\n",
      "0         0.0         0.0         0.0  -0.939152 -0.277822  -0.95157   \n",
      "1         0.0         0.0         0.0  -0.939152 -0.277822  -0.95157   \n",
      "\n",
      "   idle_min     label   label.1  \n",
      "0 -0.793988 -0.687776 -1.507472  \n",
      "1 -0.793988 -0.687776 -1.507472  \n",
      "\n",
      "[2 rows x 85 columns]\n",
      "\n",
      "Scaled KDDCup1999 DataFrame:\n",
      "     0         1         2    3         4    5    6    7    8    9  ...  \\\n",
      "0  0.0 -0.535962 -0.588439  0.0  1.021557  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "1  0.0 -0.535962 -0.588439  0.0  1.021557  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "\n",
      "         32        33        34        35   36   37   38   39   40        41  \n",
      "0 -0.817046 -0.825064  0.362713 -0.327479  0.0  0.0  0.0  0.0  0.0  0.854593  \n",
      "1 -0.712453 -0.718398  0.362713 -0.238936  0.0  0.0  0.0  0.0  0.0  0.854593  \n",
      "\n",
      "[2 rows x 42 columns]\n",
      "\n",
      "Scaled NSL-KDD DataFrame:\n",
      "          0       tcp   private       rej       0.1       0.2  0.3  0.4  0.5  \\\n",
      "0 -0.181192 -0.176253  0.936992 -1.829661 -0.179037 -0.368555  0.0  0.0  0.0   \n",
      "1 -0.179372 -0.176253 -0.882484  0.662399  0.241165 -0.368555  0.0  0.0  0.0   \n",
      "\n",
      "        0.6  ...    0.04.1    0.06.1      0.22      0.23      0.24      0.25  \\\n",
      "0 -0.213803  ... -1.397366 -0.138372 -0.431900 -0.305722 -0.358002 -0.352634   \n",
      "1 -0.213803  ...  0.002682 -0.228978  1.559641  0.072757 -0.358002 -0.352634   \n",
      "\n",
      "        1.2       1.3   neptune        21  \n",
      "0  1.979539  1.928873 -0.243035  0.698168  \n",
      "1 -0.602729 -0.565464  0.045364  0.698168  \n",
      "\n",
      "[2 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Encode categorical features\n",
    "def encode_categorical(df):\n",
    "    label_encoders = {}\n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "    return df, label_encoders\n",
    "\n",
    "cicids2017_df, cicids2017_encoders = encode_categorical(cicids2017_df)\n",
    "kddcup1999_df, kddcup1999_encoders = encode_categorical(kddcup1999_df)\n",
    "nsl_kdd_df, nsl_kdd_encoders = encode_categorical(nsl_kdd_df)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "cicids2017_scaled = pd.DataFrame(scaler.fit_transform(cicids2017_df), columns=cicids2017_df.columns)\n",
    "kddcup1999_scaled = pd.DataFrame(scaler.fit_transform(kddcup1999_df), columns=kddcup1999_df.columns)\n",
    "nsl_kdd_scaled = pd.DataFrame(scaler.fit_transform(nsl_kdd_df), columns=nsl_kdd_df.columns)\n",
    "\n",
    "print(\"Scaled CICIDS2017 DataFrame:\")\n",
    "print(cicids2017_scaled.head(2))\n",
    "print(\"\\nScaled KDDCup1999 DataFrame:\")\n",
    "print(kddcup1999_scaled.head(2))\n",
    "print(\"\\nScaled NSL-KDD DataFrame:\")\n",
    "print(nsl_kdd_scaled.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Merging**\n",
    "\n",
    "To create a comprehensive model, we'll combine the datasets into a single DataFrame. This process involves concatenating the data and ensuring consistency in features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n",
      "    flow_id    src_ip  src_port    dst_ip  dst_port  protocol  timestamp  \\\n",
      "0 -0.686773 -0.417225  0.966619  0.452353 -0.667080 -0.865321   0.769594   \n",
      "1 -0.686728 -0.417225  0.966671  0.452353 -0.667080 -0.865321   0.769594   \n",
      "2 -0.686683 -0.417225  0.966722  0.452353 -0.667080 -0.865321   0.769594   \n",
      "3 -0.325367 -0.417225  0.554961  1.551699 -0.667080 -0.865321   0.769594   \n",
      "4 -0.999105 -0.417225 -0.185706 -0.353687  0.238927 -0.865321   0.769388   \n",
      "\n",
      "   flow_duration  total_fwd_packet  total_bwd_packets  ...  10.1  0.04.1  \\\n",
      "0      -0.533407         -0.162008          -0.154376  ...   NaN     NaN   \n",
      "1      -0.533402         -0.162008          -0.154376  ...   NaN     NaN   \n",
      "2      -0.533402         -0.162008          -0.154376  ...   NaN     NaN   \n",
      "3      -0.533404         -0.162008          -0.154376  ...   NaN     NaN   \n",
      "4      -0.249732          1.868664           1.175566  ...   NaN     NaN   \n",
      "\n",
      "   0.06.1  0.22  0.23  0.24  0.25  1.2  1.3  neptune  \n",
      "0     NaN   NaN   NaN   NaN   NaN  NaN  NaN      NaN  \n",
      "1     NaN   NaN   NaN   NaN   NaN  NaN  NaN      NaN  \n",
      "2     NaN   NaN   NaN   NaN   NaN  NaN  NaN      NaN  \n",
      "3     NaN   NaN   NaN   NaN   NaN  NaN  NaN      NaN  \n",
      "4     NaN   NaN   NaN   NaN   NaN  NaN  NaN      NaN  \n",
      "\n",
      "[5 rows x 167 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143030 entries, 0 to 143029\n",
      "Columns: 167 entries, flow_id to neptune\n",
      "dtypes: float64(166), object(1)\n",
      "memory usage: 182.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Add a column to identify the source dataset\n",
    "cicids2017_scaled['source'] = 'CICIDS2017'\n",
    "kddcup1999_scaled['source'] = 'KDDCup1999'\n",
    "nsl_kdd_scaled['source'] = 'NSL-KDD'\n",
    "\n",
    "# Concatenate data\n",
    "merged_df = pd.concat([cicids2017_scaled, kddcup1999_scaled, nsl_kdd_scaled], ignore_index=True)\n",
    "\n",
    "print(\"Merged DataFrame:\")\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Building and Evaluation**\n",
    "\n",
    "We will split the data into training and test sets, build several machine learning models, and evaluate their performance. This includes:\n",
    "- Splitting the data\n",
    "- Training and evaluating models\n",
    "- Comparing model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Model Building and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CICIDS2017 Columns:\n",
      "Index(['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n",
      "       'Timestamp', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
      "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
      "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
      "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
      "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
      "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
      "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
      "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
      "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
      "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
      "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
      "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
      "       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
      "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
      "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
      "       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
      "       'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
      "       'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
      "       'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
      "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
      "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
      "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
      "       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n",
      "       'Idle Min', 'Label', 'Label.1'],\n",
      "      dtype='object')\n",
      "\n",
      "KDDCup1999 Columns:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
      "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n",
      "       '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36',\n",
      "       '37', '38', '39', '40', '41'],\n",
      "      dtype='object')\n",
      "\n",
      "NSL-KDD Columns:\n",
      "Index(['0', 'tcp', 'private', 'rej', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6',\n",
      "       '0.7', '0.8', '0.9', '0.10', '0.11', '0.12', '0.13', '0.14', '0.15',\n",
      "       '0.16', '0.17', '0.18', '229', '10', '0.19', '0.20', '1', '1.1', '0.04',\n",
      "       '0.06', '0.21', '255', '10.1', '0.04.1', '0.06.1', '0.22', '0.23',\n",
      "       '0.24', '0.25', '1.2', '1.3', 'neptune', '21'],\n",
      "      dtype='object')\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "           5       0.00      0.00      0.00       1.0\n",
      "           6       0.00      0.00      0.00       1.0\n",
      "           9       0.00      0.00      0.00       1.0\n",
      "          15       0.00      0.00      0.00       1.0\n",
      "          16       0.00      0.00      0.00       1.0\n",
      "          18       0.00      0.00      0.00       0.0\n",
      "          20       0.00      0.00      0.00       1.0\n",
      "          21       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       9.0\n",
      "   macro avg       0.00      0.00      0.00       9.0\n",
      "weighted avg       0.00      0.00      0.00       9.0\n",
      "\n",
      "Random Forest Accuracy Score: 0.0\n",
      "\n",
      "Support Vector Machine Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "           5       0.00      0.00      0.00       1.0\n",
      "           6       0.00      0.00      0.00       1.0\n",
      "           9       0.00      0.00      0.00       1.0\n",
      "          15       0.00      0.00      0.00       1.0\n",
      "          16       0.00      0.00      0.00       1.0\n",
      "          19       0.00      0.00      0.00       0.0\n",
      "          20       0.00      0.00      0.00       1.0\n",
      "          21       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       9.0\n",
      "   macro avg       0.00      0.00      0.00       9.0\n",
      "weighted avg       0.00      0.00      0.00       9.0\n",
      "\n",
      "SVM Accuracy Score: 0.0\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "           5       0.00      0.00      0.00       1.0\n",
      "           6       0.00      0.00      0.00       1.0\n",
      "           9       0.00      0.00      0.00       1.0\n",
      "          15       0.00      0.00      0.00       1.0\n",
      "          16       0.00      0.00      0.00       1.0\n",
      "          18       0.00      0.00      0.00       0.0\n",
      "          20       0.00      0.00      0.00       1.0\n",
      "          21       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       9.0\n",
      "   macro avg       0.00      0.00      0.00       9.0\n",
      "weighted avg       0.00      0.00      0.00       9.0\n",
      "\n",
      "Logistic Regression Accuracy Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load datasets\n",
    "cicids2017_df = pd.read_csv('E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\processedCICIDS2017_processed.csv')\n",
    "kddcup1999_df = pd.read_csv('E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\KDDCup1999_processed.csv')\n",
    "nsl_kdd_df = pd.read_csv('E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\data\\\\processed\\\\NSL-KDD_processed.csv')\n",
    "\n",
    "# Check column names for each dataset\n",
    "print(\"CICIDS2017 Columns:\")\n",
    "print(cicids2017_df.columns)\n",
    "print(\"\\nKDDCup1999 Columns:\")\n",
    "print(kddcup1999_df.columns)\n",
    "print(\"\\nNSL-KDD Columns:\")\n",
    "print(nsl_kdd_df.columns)\n",
    "\n",
    "# Rename target columns for consistency\n",
    "cicids2017_df['target'] = cicids2017_df['Label']  # Assuming 'Label' is the actual target column\n",
    "kddcup1999_df['target'] = kddcup1999_df['41']     # Assuming '41' is the actual target column\n",
    "nsl_kdd_df['target'] = nsl_kdd_df['21']           # Assuming '21' is the actual target column\n",
    "\n",
    "# Drop unnecessary or duplicate columns\n",
    "cicids2017_df.drop(['Label', 'Label.1'], axis=1, inplace=True)  # Drop original target columns\n",
    "kddcup1999_df.drop(['41'], axis=1, inplace=True)               # Drop original target column\n",
    "nsl_kdd_df.drop(['21'], axis=1, inplace=True)                  # Drop original target column\n",
    "\n",
    "# Add a column to identify the source dataset\n",
    "cicids2017_df['source'] = 'CICIDS2017'\n",
    "kddcup1999_df['source'] = 'KDDCup1999'\n",
    "nsl_kdd_df['source'] = 'NSL-KDD'\n",
    "\n",
    "# Concatenate data\n",
    "merged_df = pd.concat([cicids2017_df, kddcup1999_df, nsl_kdd_df], ignore_index=True)\n",
    "\n",
    "# Drop columns with excessive missing values or irrelevance\n",
    "merged_df.dropna(axis=1, thresh=0.9*len(merged_df), inplace=True)\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Ensure target column is categorical\n",
    "merged_df['target'] = merged_df['target'].astype('category').cat.codes\n",
    "\n",
    "# Split data into features and target\n",
    "X = merged_df.drop('target', axis=1)\n",
    "y = merged_df['target']\n",
    "\n",
    "# Convert categorical columns to numerical\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Ensure all feature columns are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, force errors to NaN\n",
    "\n",
    "# Fill any remaining NaN values with a placeholder or remove\n",
    "X.fillna(0, inplace=True)  # or use X.dropna(inplace=True) if you prefer\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', random_state=42)  # Linear kernel for simplicity\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)  # Increased max_iter for convergence\n",
    "\n",
    "# Train models\n",
    "rf_model.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "\n",
    "# Print classification reports and accuracy scores\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(\"Random Forest Accuracy Score:\", accuracy_score(y_test, rf_predictions))\n",
    "\n",
    "print(\"\\nSupport Vector Machine Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "print(\"SVM Accuracy Score:\", accuracy_score(y_test, svm_predictions))\n",
    "\n",
    "print(\"\\nLogistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, lr_predictions))\n",
    "print(\"Logistic Regression Accuracy Score:\", accuracy_score(y_test, lr_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "           5       0.00      0.00      0.00       1.0\n",
      "           6       0.00      0.00      0.00       1.0\n",
      "           9       0.00      0.00      0.00       1.0\n",
      "          15       0.00      0.00      0.00       1.0\n",
      "          16       0.00      0.00      0.00       1.0\n",
      "          18       0.00      0.00      0.00       0.0\n",
      "          20       0.00      0.00      0.00       1.0\n",
      "          21       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       9.0\n",
      "   macro avg       0.00      0.00      0.00       9.0\n",
      "weighted avg       0.00      0.00      0.00       9.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Rahul\\.conda\\envs\\nettrafficguard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\model\\\\random_forest_model.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'E:\\\\Hackatone Project\\\\NetTrafficGuard\\\\model\\\\random_forest_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Hyperparameter Tuning\n",
    "Hyperparameter tuning can significantly improve the performance of your model. Use techniques such as Grid Search or Random Search to find the best parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes in y_train: 21\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "An error occurred: n_splits=3 cannot be greater than the number of members in each class.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Ensure y_train is defined and check the number of unique classes\n",
    "if 'y_train' not in globals():\n",
    "    raise ValueError(\"y_train is not defined. Ensure the target variable is properly set.\")\n",
    "\n",
    "# Check the number of unique classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(f\"Number of unique classes in y_train: {num_classes}\")\n",
    "\n",
    "# Define a cross-validation strategy, ensuring n_splits does not exceed the number of unique classes\n",
    "cv_strategy = StratifiedKFold(n_splits=min(3, num_classes))\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_strategy, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "try:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    # Print the best parameters and best score\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\\nBest Score:\")\n",
    "    print(grid_search.best_score_)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 . Feature Importance Analysis\n",
    "Analyzing feature importance helps in understanding which features contribute most to the models predictions. This can guide further feature engineering and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "target\n",
      "22    1\n",
      "3     1\n",
      "12    1\n",
      "14    1\n",
      "11    1\n",
      "27    1\n",
      "13    1\n",
      "4     1\n",
      "19    1\n",
      "2     1\n",
      "25    1\n",
      "29    1\n",
      "10    1\n",
      "24    1\n",
      "23    1\n",
      "8     1\n",
      "7     1\n",
      "18    1\n",
      "26    1\n",
      "17    1\n",
      "28    1\n",
      "Name: count, dtype: int64\n",
      "Using 2 splits for cross-validation.\n",
      "Fitting 21 folds for each of 108 candidates, totalling 2268 fits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_train and y_train are already defined and preprocessed\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Check the number of samples per class\n",
    "class_counts = y_train.value_counts()\n",
    "print(\"Class distribution:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Determine a valid number of splits\n",
    "min_class_count = class_counts.min()\n",
    "num_splits = max(2, min_class_count)  # Ensure splits are at least 2 but not more than the smallest class size\n",
    "\n",
    "print(f\"Using {num_splits} splits for cross-validation.\")\n",
    "\n",
    "# Define a cross-validation strategy\n",
    "if num_splits > 2:\n",
    "    cv_strategy = StratifiedKFold(n_splits=num_splits)\n",
    "else:\n",
    "    cv_strategy = LeaveOneOut()  # Fallback to LOOCV if too few samples\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_strategy, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "try:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best parameters and best score\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\\nBest Score:\")\n",
    "    print(grid_search.best_score_)\n",
    "\n",
    "    # Use the best model from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get feature importances from the best model\n",
    "    importances = best_model.feature_importances_\n",
    "\n",
    "    # Ensure feature_names is defined and contains the feature names\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        feature_names = X_train.columns\n",
    "    else:\n",
    "        raise ValueError(\"X_train is not a DataFrame. Ensure X_train is properly defined.\")\n",
    "\n",
    "    # Create a DataFrame for feature importances\n",
    "    features_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=features_df)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError occurred: {e}\")\n",
    "    print(\"Consider using different cross-validation techniques or adjusting the parameter grid.\")\n",
    "except NotFittedError as e:\n",
    "    print(f\"Model fitting error: {e}\")\n",
    "    print(\"The model might not be fitted properly. Check the data and model configuration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Conclusion\n",
    "In this notebook, we have:\n",
    "- Loaded and combined datasets from KDDCup1999, NSL-KDD, and CICIDS2017.\n",
    "- Performed data preprocessing including handling missing values and feature selection.\n",
    "- Developed and evaluated a RandomForest model for network traffic analysis.\n",
    "- Saved the trained model for future use.\n",
    "\n",
    "The next steps could involve fine-tuning the model, exploring additional features, or integrating the model into the Django application for real-time predictions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
